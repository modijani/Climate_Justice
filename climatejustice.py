# -*- coding: utf-8 -*-
"""climatejustice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NqdameIBkBE9uGvV4kzJHMVWsIs6BESE

Data is here: https://drive.google.com/drive/folders/1NzKJRbrj-3f7NuWVrKVJKy5r-crKeO3C?usp=sharing
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os
import seaborn as sb
import matplotlib.pyplot as plt
!pip install geopandas

tri_path = 'drive/MyDrive/bigdata_final/tri_data/'
race_path = 'drive/MyDrive/bigdata_final/race_data/'
map_path = 'drive/MyDrive/bigdata_final/map_data/cb_2018_us_county_5m.shp'

dict_df_race = {}
for file_name in os.listdir(race_path):
    if file_name.endswith('.csv'):
      file_name_clean = file_name[:-4]
      file_path = os.path.join(race_path, file_name)
      df = pd.read_csv(file_path, index_col=0, encoding='latin-1')
      dict_df_race[file_name_clean] = df

#find the percentage of white people in yr 2021
file_path = os.path.join(race_path, "2021.csv")
df_race_2021 = pd.read_csv(file_path, index_col=0, encoding='latin-1')
df_race_2021.to_csv('output.csv', index=False)
df_race_2021['percent_poc'] = (df_race_2021['ANLTE001'] - df_race_2021['ANLTE002']) / df_race_2021['ANLTE001']
df_race_2021['COUNTY'] = df_race_2021['COUNTY'].str.replace(' County', '')
df_race_2021['COUNTY'] = df_race_2021['COUNTY'].str.upper()

df_race_2021 = df_race_2021[['COUNTY', 'STUSAB', 'ANLTE001', 'ANLTE002', 'percent_poc']]

df_race_2021

#find the percentage of white people in yr 2021
file_path = os.path.join(race_path, "2021.csv")
df_race_2021 = pd.read_csv(file_path, index_col=0, encoding='latin-1')
df_race_2021.to_csv('output.csv', index=False)
df_race_2021['percent_poc'] = (df_race_2021['ANLTE001'] - df_race_2021['ANLTE002']) / df_race_2021['ANLTE001']
df_race_2021['COUNTY'] = df_race_2021['COUNTY'].str.replace(' County', '')
df_race_2021['COUNTY'] = df_race_2021['COUNTY'].str.upper()

df_race_2021 = df_race_2021[['COUNTY', 'STUSAB', 'ANLTE001', 'ANLTE002', 'percent_poc']]

df_race_2021

#find the total pollution in yr 2021
file_path = os.path.join(tri_path, "2021.csv")
df_tri_2021 = pd.read_csv(file_path, index_col=0, encoding='latin-1')

file_path = os.path.join(tri_path, "2021.csv")
df_tri_2021 = pd.read_csv(file_path, index_col=0, encoding='latin-1')

df_tri_2021[['7. COUNTY','8. ST', '104. TOTAL RELEASES']]

df_tri_2021 = df_tri_2021.groupby(by=["7. COUNTY", '8. ST'])['104. TOTAL RELEASES'].sum().reset_index(name ='Total Release')
df_tri_2021

"""## First, we plot the 2021 data from the csv files. We show that there is decreased air quality in areas with more POC."""

# merge the two DataFrames on 'COUNTY' and '8. ST' columns
merged = pd.merge(df_race_2021, df_tri_2021, left_on=['COUNTY', 'STUSAB'], right_on=['7. COUNTY', '8. ST'], how='inner')
merged['percent_poc'] = merged['percent_poc']*100

# create scatter plot of percent_poc vs Total Release
plt.scatter(merged['percent_poc'], merged['Total Release'], alpha=0.2)
plt.xlabel('Percent POC')
plt.ylabel('Total Release')
#plt.yscale('log')
#plt.xscale('log')

plt.show()

"""You can already see that the Y axis has some outlier, we removed those by only taking data believe 1e8.
This shows some of the X axis outliers. As you can see, only a few counties have a poc percentage above 80% so we removed those as well.
"""

# merge the two DataFrames on 'COUNTY' and '8. ST' columns
merged_outlier_x = merged[merged['percent_poc'] > 60]

# create scatter plot of percent_poc vs Total Release
plt.scatter(merged_outlier_x['percent_poc'], merged_outlier_x['Total Release'])
plt.xlabel('Percent POC')
plt.ylabel('Total Release')
plt.show()

# merge the two DataFrames on 'COUNTY' and '8. ST' columns
merged = merged[merged['percent_poc'] < 55]
merged = merged[merged['Total Release'] < .5e8]

# create bins for percent_poc column
bins = pd.cut(merged['percent_poc'], bins=range(0, 101, 3))

# group data by bins and calculate mean of Total Release for each group
grouped = merged.groupby(bins)['Total Release'].mean()

# convert index (pandas Interval objects) to strings
grouped.index = grouped.index.astype(str)

# create scatter plot of percent_poc vs Total Release with size (10, 6)
fig, ax = plt.subplots(figsize=(18,6))
ax.scatter(grouped.index, grouped.values)
ax.set_xlabel('Percent POC')
ax.set_ylabel('Total Release')
plt.show()

"""# Look at the data across 10 years"""

dict_df_race = {}
dict_df_tri = {}

total_keys = {"2010": "H7X001", "2011": "LJCE001", "2012": "OJFE001", "2013": "SBME001", "2014": "AAA6E001", "2015": "ACK3E001", "2016": "AE2CE001", "2017": "AGXGE001", "2018": "AIUXE001", "2019": "AKSME001", "2020": "U7B001", "2021": "ANLTE001"}
white_keys = {"2010": "H7X002", "2011": "LJCE002", "2012": "OJFE002", "2013": "SBME002", "2014": "AAA6E002", "2015": "ACK3E002", "2016": "AE2CE002", "2017": "AGXGE002", "2018": "AIUXE002", "2019": "AKSME002", "2020": "U7B003", "2021": "ANLTE002"}

for file_name in os.listdir(race_path):
    if file_name.endswith('.csv'):
      file_name_clean = file_name[:-4]
      file_path = os.path.join(race_path, file_name)
      df = pd.read_csv(file_path, index_col=0, encoding='latin-1')
      #find the percentage of white people in yr
      df['percent_poc'] = (df[total_keys[file_name_clean]] - df[white_keys[file_name_clean]]) / df[total_keys[file_name_clean]]
      df['COUNTY'] = df['COUNTY'].str.replace(' County', '')
      df['COUNTY'] = df['COUNTY'].str.upper()

      df = df[['COUNTY', 'STUSAB', total_keys[file_name_clean], white_keys[file_name_clean], 'percent_poc']]
      df['percent_poc'] = df['percent_poc']*100
      df = df[df['percent_poc'] < 55]
      dict_df_race[file_name_clean] = df


for file_name in os.listdir(tri_path):
    if file_name.endswith('.csv'):
      file_name_clean = file_name[:-4]
      file_path = os.path.join(tri_path, file_name)
      df = pd.read_csv(file_path, index_col=0, encoding='latin-1')

      #find the total pollution in yr
      df[['7. COUNTY','8. ST', '104. TOTAL RELEASES']]
      df = df.groupby(by=["7. COUNTY", '8. ST'])['104. TOTAL RELEASES'].sum().reset_index(name ='Total Release')
      df = df[df['Total Release'] < .5e8]
      dict_df_tri[file_name_clean] = df

dict_df_race = dict(sorted(dict_df_race.items()))
dict_df_tri = dict(sorted(dict_df_tri.items()))

import matplotlib.pyplot as plt


for year, race_df in dict_df_race.items():
  if year in dict_df_tri:
    tri_df = dict_df_tri[year]
    # merge the two DataFrames on 'COUNTY' and '8. ST' columns
    merged = pd.merge(race_df, tri_df, left_on=['COUNTY', 'STUSAB'], right_on=['7. COUNTY', '8. ST'], how='inner')

    # merged = merged[merged['Total Release'] < 1e8].all()

    # create bins for percent_poc column
    bins = pd.cut(merged['percent_poc'], bins=range(0, 101, 3))

    # group data by bins and calculate mean of Total Release for each group
    grouped = merged.groupby(bins)['Total Release'].mean()

    # convert index (pandas Interval objects) to strings
    grouped.index = grouped.index.astype(str)

    # create scatter plot of percent_poc vs Total Release with size (10, 6)
    fig, ax = plt.subplots(figsize=(18,6))
    ax.scatter(grouped.index, grouped.values)
    ax.set_xlabel('Percent POC')
    ax.set_ylabel('Total Release')
    ax.set_title(year)

    plt.show()

import matplotlib.pyplot as plt
import numpy as np

dict_df_combined = {}
dict_counties = {}
years = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']

for year, race_df in dict_df_race.items():
  if year in dict_df_tri:
    tri_df = dict_df_tri[year]

    # create a new data frame for each county:
    # define a new DataFrame with two columns
    df_county = pd.DataFrame(columns=['Percent POC', 'Total Release'])

    # merge the two DataFrames on 'COUNTY' and '8. ST' columns
    merged = pd.merge(race_df, tri_df, left_on=['COUNTY', 'STUSAB'], right_on=['7. COUNTY', '8. ST'], how='inner')
    merged = merged[['COUNTY', 'STUSAB', 'percent_poc', 'Total Release']]

    for index, row in merged.iterrows():
      key_value = row.iloc[0] +", "+ row.iloc[1]
      if key_value not in dict_counties:
        new_df = pd.DataFrame(index=years, columns=['percent_poc', 'Total Release'])
        new_df.loc[year] = [row.iloc[2], row.iloc[3]]
        new_df.index.name = 'Year'
        dict_counties[key_value] = new_df
      dict_counties[key_value].loc[year, 'percent_poc'] = row.iloc[2]
      dict_counties[key_value].loc[year, 'Total Release'] = row.iloc[3]

df_all_counties = pd.DataFrame()
# concatenate all dataframes in the dictionary into one dataframe
for df in dict_counties.values():
    df_all_counties = pd.concat([df_all_counties, df])

# concatenate all dataframes in the dictionary into one dataframe
df_all_counties = pd.concat(dict_counties.values())

# group the dataframe by year and sum the Total Release column
df_all_years = df_all_counties.groupby('Year').sum()

# plot Total Release column against the index
plt.plot(df_all_years.index, df_all_years['Total Release'])

# set plot title and axis labels
plt.title('Total Toxicity over Time for All Counties')
plt.xlabel('Year')
plt.ylabel('Total Toxicity')

# display plot
plt.show()

dict_counties_smoothed = {}

# Define the window size for the moving average
window_size = 3

# Set the padding size for the symmetric padding
padding_size = 1

for county_name, county in dict_counties.items():
  removed_outliers_df = county
  # Drop the data with na
  removed_outliers_df = removed_outliers_df.dropna()

  # Remove outliers
  # Calculate the 0.01 and 0.99 quantiles
  q_low = removed_outliers_df['Total Release'].quantile(0.05)
  q_high = removed_outliers_df['Total Release'].quantile(0.95)
  q_low_poc = removed_outliers_df['percent_poc'].quantile(0.05)
  q_high_poc = removed_outliers_df['percent_poc'].quantile(0.95)

  # Filter out the rows with B values outside of the 0.01-0.99 quantile range
  removed_outliers_df = removed_outliers_df[(removed_outliers_df['Total Release'] >= q_low) & (removed_outliers_df['Total Release'] <= q_high)]
  removed_outliers_df = removed_outliers_df[(removed_outliers_df['percent_poc'] >= q_low_poc) & (removed_outliers_df['percent_poc'] <= q_high_poc)]

  # Check the length of the data before applying symmetric padding
  if len(removed_outliers_df) > padding_size:
    # Apply symmetric padding to the data
    padded_data = pd.concat([removed_outliers_df.iloc[padding_size::-1], removed_outliers_df, removed_outliers_df.iloc[:-padding_size-1:-1]])

    # Apply the moving average filter
    smoothed_data = padded_data.rolling(window_size, center=True).mean()

    # Remove the padded data from the smoothed data
    smoothed_data = smoothed_data.iloc[padding_size:-padding_size]

    dict_counties_smoothed[county_name] = smoothed_data
  else:
    # If the data length is less than or equal to the padding size, skip the smoothing process
    dict_counties_smoothed[county_name] = removed_outliers_df

# make a copy of the dictionary keys
keys = list(dict_counties_smoothed.keys())

# iterate over the dictionary keys
for key in keys:
    # check if the DataFrame is empty
    if len(dict_counties_smoothed[key]) == 0:
        # delete the key from the dictionary
        del dict_counties_smoothed[key]

for i, (county, value) in enumerate(dict_counties.items()):
    if county == 'WARREN, OH':
      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))

      # Plot percent_poc vs time
      ax1.plot(value.index, value['percent_poc'])
      ax1.set_xlabel('Year')
      ax1.set_ylabel('Percent POC')
      ax1.set_title('Warren County Percent POC vs Time')

      # Plot Total Release vs time
      ax2.plot(value.index, value['Total Release'])
      ax2.set_xlabel('Year')
      ax2.set_ylabel('Total Release (lbs)')
      ax2.set_title('Warrren County Total Release vs Time')
      plt.show()

for i, (county, value) in enumerate(dict_counties_smoothed.items()):
    if county == 'WARREN, OH':
      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))

      # Plot percent_poc vs time
      ax1.plot(value.index, value['percent_poc'])
      ax1.set_xlabel('Year')
      ax1.set_ylabel('Percent POC')
      ax1.set_title('Warren County Percent POC vs Time (post-smooth)')

      # Plot Total Release vs time
      ax2.plot(value.index, value['Total Release'])
      ax2.set_xlabel('Year')
      ax2.set_ylabel('Total Release (lbs)')
      ax2.set_title('Warren County Total Release vs Time (post-smooth)')
      plt.show()

# iterate over each county in the dict_counties dictionary
for county_name, county in dict_counties_smoothed.items():
  # create x and y arrays
  x = np.array(county.index)
  poc_y = np.array(county.loc[:, 'percent_poc'])
  tri_y = np.array(county.loc[:, 'Total Release'])

  if (len(poc_y) > 0):

    x = x.astype(float)
    poc_y = poc_y.astype(float)
    tri_y = tri_y.astype(float)

    # create a linear regression model
    poc_coefficients = np.polyfit(x, poc_y, 1)
    tri_coefficients = np.polyfit(x, tri_y, 1)

    poc_slope, poc_intercept = poc_coefficients
    tri_slope, tri_intercept = tri_coefficients

    # add slope and y inter to data frame
    county['poc_slope'] = [poc_slope] * len(county)
    county['poc_intercept'] = [poc_intercept] * len(county)
    county['tri_slope'] = [tri_slope] * len(county)
    county['tri_intercept'] = [tri_intercept] * len(county)

    # get the slope from the coefficients
    poc_slope = poc_coefficients[0]
    tri_slope = tri_coefficients[0]

print (dict_counties_smoothed)

"""Smooth Data Points

why is there always both data points but never one and not the other
"""





from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# create a new dataframe with all available data
data = pd.DataFrame()
for county, df in dict_counties_smoothed.items():
    df['County'] = county
    data = pd.concat([data, df], axis=0)

# drop missing values
data.dropna(inplace=True)

# encode county names
encoder = LabelEncoder()
data['County'] = encoder.fit_transform(data['County'])

data = data.reset_index()

data



data_for_ml = data.drop_duplicates(subset='County', keep='first')
data_for_ml = data_for_ml[['poc_slope', 'tri_slope']]

data_for_ml

# Create the dataframe
df_for_table = data_for_ml

poc_slope_avg = data_for_ml['poc_slope']
tri_slope_avg = data_for_ml['tri_slope']

print('poc_slope:', poc_slope_avg)
print('tri_slope:', tri_slope_avg)

# Create a new column to indicate the direction of each slope
df_for_table['poc_dir'] = df_for_table['poc_slope'].apply(lambda x: 'more' if x > poc_slope_avg else 'less')
df_for_table['tri_dir'] = df_for_table['tri_slope'].apply(lambda x: 'more' if x > tri_slope_avg else 'less')

# Create the contingency table
table = pd.crosstab(df_for_table['poc_dir'], df_for_table['tri_dir'])

# Print the table
print(table)

# Input data
poc_slope = data_for_ml['poc_slope']
tri_slope = data_for_ml['tri_slope']

# Initialize counters
pos_pos = 0
pos_neg = 0
neg_pos = 0
neg_neg = 0

# Iterate through the data and count the categories
for poc, tri in zip(poc_slope, tri_slope):
    if poc > 0 and tri > 0:
        pos_pos += 1
    elif poc > 0 and tri < 0:
        pos_neg += 1
    elif poc < 0 and tri > 0:
        neg_pos += 1
    elif poc < 0 and tri < 0:
        neg_neg += 1

# Print the results in a table
print("|               | Positive TRI | Negative TRI |")
print("|-------------- |--------------|------------- |")
print(f"| Positive %POC | {pos_pos}          | {pos_neg}          |")
print(f"| Negative %POC | {neg_pos}           | {neg_neg}           |")

import seaborn as sns

# Calculate the IQR of each column
poc_iqr = data_for_ml['poc_slope'].quantile(0.75) - data_for_ml['poc_slope'].quantile(0.25)
tri_iqr = data_for_ml['tri_slope'].quantile(0.75) - data_for_ml['tri_slope'].quantile(0.25)

# Calculate the outlier thresholds
poc_threshold = poc_iqr * 1.5
tri_threshold = tri_iqr * 1.5

# Remove the outliers
df = data_for_ml[(data_for_ml['poc_slope'] >= df['poc_slope'].quantile(0.25) - poc_threshold) & (data_for_ml['poc_slope'] <= data_for_ml['poc_slope'].quantile(0.75) + poc_threshold)]
df = df[(data_for_ml['tri_slope'] >= df['tri_slope'].quantile(0.25) - tri_threshold) & (df['tri_slope'] <= df['tri_slope'].quantile(0.75) + tri_threshold)]

# Create the scatter plot without outliers
# perform linear regression
X = df[['poc_slope']]
y = df['tri_slope']
regressor = LinearRegression()
regressor.fit(X, y)

# print the slope and intercept of the line of best fit
print('Slope:', regressor.coef_[0])
print('Intercept:', regressor.intercept_)

# calculate R-squared value
r_squared = regressor.score(X, y)

# plot the scatter plot with line of best fit and R-squared value
sns.scatterplot(data=df, x='poc_slope', y='tri_slope', alpha=0.2)
plt.plot(X, regressor.predict(X), color='red', linewidth=2)
plt.text(0.005, 18, f'R-squared = {r_squared:.2f}', fontsize=10, color='red')
plt.show()

# calculate the average of poc_slope and tri_slope
poc_slope_avg = df['poc_slope'].mean()
tri_slope_avg = df['tri_slope'].mean()

print(f"The average poc_slope is {poc_slope_avg:.4f}.")
print(f"The average tri_slope is {tri_slope_avg:.4f}.")

# calculate the number of rows in the original and filtered dataframes
num_rows_original = data_for_ml.shape[0]
num_rows_filtered = df.shape[0]

# calculate the percentage of data being filtered out
percent_filtered = (num_rows_original - num_rows_filtered) / num_rows_original * 100
print (percent_filtered)

# split the data into training and testing sets
X = data_for_ml[['tri_slope']]
y = data_for_ml['poc_slope']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Evaluate the model on the testing data
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
print(f"MSE: {mse:.2f}, RMSE: {rmse:.2f}, R-squared: {r2:.2f}")

data = pd.DataFrame()
for county, df in dict_counties_smoothed.items():
    df['County'] = county
    data = pd.concat([data, df], axis=0)

data_for_map = data

data_for_fips = {'fips_code': [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56],
        'state_abbr': ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']}


fips_dict = dict(zip(data_for_fips['state_abbr'], data_for_fips['fips_code']))
print(fips_dict)

data_for_map

# Create a new column 'State' by splitting the 'County' column
data_for_map['State'] = data_for_map['County'].str.split(', ').str[1]

# Create a new column 'County' by taking the first part of the 'County' column
data_for_map['County'] = data_for_map['County'].str.split(', ').str[0]

data_for_map['FIPS'] = data_for_map['State'].map(fips_dict)

#data_for_map['FIPS'] = data_for_map['FIPS'].astype(int)

data_for_map

data_for_map_2 = data_for_map
data_for_map_2 = data_for_map_2.dropna(subset=['FIPS'])
data_for_map_2['FIPS'] = data_for_map_2['FIPS'].astype(int)

data_for_map_2 = data_for_map_2.reset_index()

# Sort the DataFrame by 'Year' in descending order
df_latest = data_for_map_2.sort_values('Year', ascending=False)

# Keep only the latest year for each county
df_latest = df_latest.drop_duplicates(subset=['County'], keep='first')

df_latest

import geopandas as gpd

counties = gpd.read_file(map_path)

counties = counties.rename(columns={'NAME': 'County'})
counties = counties.rename(columns={'STATEFP': 'FIPS'})
counties['FIPS'] = counties['FIPS'].astype('int64')

counties['County'] = counties['County'].str.upper()

counties

merged_df = counties.merge(df_latest, on=['County', 'FIPS'])


merged_df

# Read the data frame into a GeoDataFrame
gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')

# Create a larger figure
fig, ax = plt.subplots(figsize=(20, 20))

# Plot the choropleth map
gdf.plot(column='percent_poc', cmap='OrRd', legend=False, ax=ax)

# Add title and axis labels
plt.title('Percentage of people of color by county')
plt.xlabel('Longitude')
plt.ylabel('Latitude')

# Remove the axis frame and display the map
plt.axis('off')
plt.show()

data_for_map_change = pd.DataFrame()
for county, df in dict_counties_smoothed.items():
    df['County'] = county
    data_for_map_change = pd.concat([data_for_map_change, df], axis=0)

# drop missing values
data_for_map_change.dropna(inplace=True)

data_for_map_change_slope = data_for_map_change



data_for_map_change_slope = data_for_map_change_slope.drop_duplicates(subset='County', keep='first')

data_for_map_change_slope

# Create a new column 'State' by splitting the 'County' column
data_for_map_change_slope['State'] = data_for_map_change_slope['County'].str.split(', ').str[1]

# Create a new column 'County' by taking the first part of the 'County' column
data_for_map_change_slope['County'] = data_for_map_change_slope['County'].str.split(', ').str[0]

data_for_map_change_slope['FIPS'] = data_for_map_change_slope['State'].map(fips_dict)

#data_for_map['FIPS'] = data_for_map['FIPS'].astype(int)

data_for_map_change_slope

data_for_map_change_slope = data_for_map_change_slope.dropna(subset=['FIPS'])
data_for_map_change_slope['FIPS'] = data_for_map_change_slope['FIPS'].astype(int)


# calculate the slope_ratio column
data_for_map_change_slope['slope_ratio'] = data_for_map_change_slope['poc_slope'] / data_for_map_change_slope['tri_slope']


# Assuming your dataframe is named 'df'
data_for_map_change_slope = data_for_map_change_slope.dropna()  # Remove any rows with NaN values
data_for_map_change_slope = data_for_map_change_slope[np.isfinite(data_for_map_change_slope['slope_ratio'])]

data_for_map_change_slope

merged_df = counties.merge(data_for_map_change_slope, on=['County', 'FIPS'])


merged_df

# create a new column for the color based on the slope ratio
merged_df['color'] = ['red' if x < 0 else 'green' for x in merged_df['slope_ratio']]

# plot the data with the color scale
fig, ax = plt.subplots(figsize=(10, 10))
merged_df.plot(column='slope_ratio', cmap='coolwarm', ax=ax, legend=True)
ax.set_axis_off()
plt.show()

merged_df['is_positive_slope'] = merged_df['slope_ratio'].apply(lambda x: x > 0)

# Create a chloropleth map
fig, ax = plt.subplots(figsize=(12,8))
merged_df.plot(column='is_positive_slope', cmap='coolwarm', linewidth=0.5, ax=ax, edgecolor='gray')
ax.axis('off')
ax.set_title('Toxicity/POC correlation by County ')
plt.show()

